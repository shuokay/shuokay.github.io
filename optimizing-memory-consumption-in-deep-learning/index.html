<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"shuokay.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="翻译自 MXNet 官方文档.在深度学习中，我们总是希望能够训练更大，更深的网络模型。虽然硬件的发展速度很快，但是，为了构建更复杂的深层网络模型，总是希望有更多的显存&#x2F;内存可用。对于同样的一个网络，如果能够使用更少的内存占用，也就意味着在训练的时候可以使用更大的 batch size, 通常也意味着更高的 GPU 使用率。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习中内存占用的优化">
<meta property="og:url" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/index.html">
<meta property="og:site_name" content="Memo">
<meta property="og:description" content="翻译自 MXNet 官方文档.在深度学习中，我们总是希望能够训练更大，更深的网络模型。虽然硬件的发展速度很快，但是，为了构建更复杂的深层网络模型，总是希望有更多的显存&#x2F;内存可用。对于同样的一个网络，如果能够使用更少的内存占用，也就意味着在训练的时候可以使用更大的 batch size, 通常也意味着更高的 GPU 使用率。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/comp_graph_example.png">
<meta property="og:image" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/back_graph.png">
<meta property="og:image" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/explicit_back_layer.png">
<meta property="og:image" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/back_dep_prune.png">
<meta property="og:image" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/back_agg_grad.png">
<meta property="og:image" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/alloc_inline.png">
<meta property="og:image" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/alloc_inline_trap.png">
<meta property="og:image" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/alloc_normal.png">
<meta property="og:image" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/alloc_mlp.png">
<meta property="og:image" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/alloc_normal.png">
<meta property="og:image" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/alloc_step.png">
<meta property="og:image" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/parallel_alloc.png">
<meta property="og:image" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/graph_color.png">
<meta property="article:published_time" content="2017-03-08T13:22:31.000Z">
<meta property="article:modified_time" content="2022-08-15T14:43:10.262Z">
<meta property="article:author" content="yushu.gao">
<meta property="article:tag" content="MXNet">
<meta property="article:tag" content="HPC">
<meta property="article:tag" content="Framework">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/comp_graph_example.png">

<link rel="canonical" href="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习中内存占用的优化 | Memo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Memo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://shuokay.com/optimizing-memory-consumption-in-deep-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="yushu.gao">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Memo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习中内存占用的优化
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-03-08 21:22:31" itemprop="dateCreated datePublished" datetime="2017-03-08T21:22:31+08:00">2017-03-08</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Framework/" itemprop="url" rel="index"><span itemprop="name">Framework</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Framework/MXNet/" itemprop="url" rel="index"><span itemprop="name">MXNet</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>翻译自 MXNet <a target="_blank" rel="noopener" href="http://mxnet.io/architecture/note_memory.html">官方文档</a>.<br>在深度学习中，我们总是希望能够训练更大，更深的网络模型。虽然硬件的发展速度很快，但是，为了构建更复杂的深层网络模型，总是希望有更多的显存&#x2F;内存可用。对于同样的一个网络，如果能够使用更少的内存占用，也就意味着在训练的时候可以使用更大的 batch size, 通常也意味着更高的 GPU 使用率。</p>
<span id="more"></span>

<p>这篇文章讨论了深度学习中内存优化的问题，并且，给出了一些可能的解决方案。这些解决方案不一定是完备的，只是表示我们对这个问题的思考。</p>
<h1 id="Computation-Graph"><a href="#Computation-Graph" class="headerlink" title="Computation Graph"></a>Computation Graph</h1><p>首先要介绍一下 CG, 后续的讨论都会基于 CG. CG 描述了在神经网络中操作之间的 (数据流) 的依赖关系。图中的操作既可以是粗粒度的也可以是细粒度的。下图给出了两个例子。</p>
<img src="/optimizing-memory-consumption-in-deep-learning/comp_graph_example.png" class="" width="600">
<p>CG 的概念来源于 Theano, CGT 中，但是，在其他的一些深度学习库中，它们也以配置文件的方式存在。在这些库之间，它们的主要却别是计算梯度的方式。主要有两种方式，一种是在原图上计算梯度，另外一种是通过明确的反向传播的路径进行计算。</p>
<img src="/optimizing-memory-consumption-in-deep-learning/back_graph.png" class="" width="600">
<p>Caffe, Torch, CXXNet 是在原图上进行计算，Theano 和 CGT 有显式的反向传播路径。在这里，我们使用显示反向传播路径的图进行讨论.(因为，对于优化问题，它有诸多有点)</p>
<p>需要强调的一点是，使用显式的反向传播路径并没有把讨论局限于符号式的库 (symbolic library), 例如 Theano, CGT. 显式反向传播图同样适用于 layer-based library(layer-based library 会把 forward 和 backward 绑定到一起). 下图展示了这个过程是如何进行的。简单来说，我们引入了一个 backward 节点，而且这个节点连接到 forward 节点，在 backward 阶段调用 <code>layer.backward</code></p>
<img src="/optimizing-memory-consumption-in-deep-learning/explicit_back_layer.png" class="" width="600">

<p>这篇讨论目前适用于几乎所有的深度学习框架 (当然会有类似于高阶微分这样的区别，但是，这些不在本文的讨论范围之内)</p>
<p>为什么说显式反向传播路径会更好呢？我们用两个例子来说明。首先，显式传播路径能够清晰的描述操作之间的依赖关系。例如下图，想计算 A 和 B 的梯度。从图中可以明确地看出，C 的梯度 <code>d(C)</code> 不依赖于 <code>F</code>, 这就意味着，对于 F 来说，一旦 forward 计算完成，可以马上释放 F 占用的内存空间。类似的 C 也可以被释放。</p>
<img src="/optimizing-memory-consumption-in-deep-learning/back_dep_prune.png" class="" width="600">
<p>第二个优点是，显式反向传播路径可以有完全不同的反向传播路径，而不仅仅是 forward 的镜像。例如分割连接的例子，如下图：</p>
<img src="/optimizing-memory-consumption-in-deep-learning/back_agg_grad.png" class="" width="600">
<p>由图可知，输出 B 被两个操作使用，如果我们想在原图中计算梯度，那么就需要引入一个 split layer, 这也以为这在 forward 的时候也许需要这样的 split layer. 在这个例子中，forward 过程没有 split layer, 但是，这个图会在把梯度传给 B 之前自动插入一个 gradient aggregation node. 这就节省了 split layer 输出的内存和 forward 过程中复制数据的操作开销。</p>
<p>如果采用显式反向传播的方式，在 forward 和 backward 过程中就没有什么差别，只需要按照时间顺序处理计算图，而且也使得本文的讨论更清晰。现在的问题是，怎样为计算图中每一个输出节点分配显存呢？</p>
<p>这个问题似乎和深度学习没有关系，更多的是和编译，数据流优化有关。主要是深度学习这个重要的问题促使我们研究这个问题，并且从中受益。</p>
<h1 id="What-Can-Be-Optimized"><a href="#What-Can-Be-Optimized" class="headerlink" title="What Can Be Optimized"></a>What Can Be Optimized</h1><p>希望读者现在能体会到计算图是讨论内存优化的一个比较好的工具。上文中已经可以看到，通过显式的反向传播路径可以实现一些内存优化，现在需要进一步探索这个问题。</p>
<p>假设我们需要构建一个 n 层的神经网络，通常来讲，需要为每一层的输出和梯度提供存储空间，错略来讲是 <code>2n</code> 的存储空间。在显式反向传播图中也是应该是大约 <code>2n</code> 的空间，因为在 backward 中的节点数和 forward 中的节点数大致相等。</p>
<h2 id="In-Place-Operations"><a href="#In-Place-Operations" class="headerlink" title="In-Place Operations"></a>In-Place Operations</h2><p>内存优化中首先第一件可以做的事情是，对于 In-Place 的 operation 可以进行内存共享。通常对于简单的激活函数可以使用这个策略。例如下面这个例子，目的是计算三个链式的 sigmoid 激活函数。</p>
<img src="/optimizing-memory-consumption-in-deep-learning/alloc_inline.png" class="" width="600">
<p>对于 sigmoid 函数可以进行 In-Place 计算，也就是说，输入和输入可以共享同一块内存，这时，只需要分配一块内存，就可以计算任意长度的链式激活函数。</p>
<p>然而，In-Place 的优化在某些情况下也会遇到问题。例如下面的例子，B 不仅被 C 使用，而且还会被 F 用到。</p>
<img src="/optimizing-memory-consumption-in-deep-learning/alloc_inline_trap.png" class="" width="600">
<p>在计算过 C&#x3D;sigmoid(B) 之后，B 仍然会被其他节点用到，因此，不能使用 In-Place 优化。因此，在代码中不能遇到 sigmoid 就使用 In-Place 优化，需要认真分析操作依赖。</p>
<h2 id="Standard-Memory-Sharing"><a href="#Standard-Memory-Sharing" class="headerlink" title="Standard Memory Sharing"></a>Standard Memory Sharing</h2><p>除了 In-Place operator, 还有其它的内存共享的方法。在下面的这个例子中，计算 E 的时候不需要 B 参与，因此，可以把 E 的计算结果放到 B 中。</p>
<img src="/optimizing-memory-consumption-in-deep-learning/alloc_normal.png" class="" width="600">
<p>内存共享并不要求数据的内存占用是相同的，只需要在分配的时候，分配内存占用量的上确界，然后，所有操作共享这块内存。</p>
<h2 id="Example-of-Real-Neural-Network-Allocation"><a href="#Example-of-Real-Neural-Network-Allocation" class="headerlink" title="Example of Real Neural Network Allocation"></a>Example of Real Neural Network Allocation</h2><p>上面的例子都是为了说明概念，生造出来的一些例子。但是，方法同样可以应用到神经网络中。下图是一个两层的感知机的内存分配：</p>
<img src="/optimizing-memory-consumption-in-deep-learning/alloc_mlp.png" class="" width="600">
<p>在上面这个例子中：</p>
<ul>
<li><code>act1</code>, <code>d(fc1)</code>, <code>out</code> 和 <code>d(fc2)</code> 是 In-Place 操作</li>
<li><code>d(act1)</code> 和 <code>d(A)</code> 共享内存。</li>
</ul>
<h1 id="Memory-Allocation-Algorithm"><a href="#Memory-Allocation-Algorithm" class="headerlink" title="Memory Allocation Algorithm"></a>Memory Allocation Algorithm</h1><p>到此为止，我们已经讨论过了内存优化的一些常见方法。其中有一些问题需要注意，例如在 In-Place 操作中提到的一个当前节点被多个其它节点依赖的情况。现在，方案是有了，具体要怎么实施呢？这并不是一个全新的问题，例如在编译器的寄存器的分配中就有类似的问题，因此，我们可以借鉴其中的一些思路。本文不会对该技术做非常详尽的分析，只是介绍一些非常有用的技巧来搞定内存优化的问题。</p>
<p>解决这个问题的核心是解决资源的冲突问题。具体来讲，就是任何一个变量都有一个生命周期 (life time), 即其第一次进行计算到其最后一次被使用到。在多层感知机的例子中，<code>fc1</code> 的生命周期在 <code>act1</code> 计算完成之后就结束了。</p>
<img src="/optimizing-memory-consumption-in-deep-learning/alloc_normal.png" class="" width="600">
<p>内存共享的原则是共享内存的变量的生命周期不会相互重叠。有多种方法可以实现这个目的。例如可以通过构建冲突图的方法，图中的每一个节点表示一个变量，生命周期有重叠的变量之间相互连接，然后在这个图上运行图染色算法，时间复杂度大约是 \(O(n^2)\), 其中，n 是图中节点的个数。这个计算复杂度还是比较大的。<br>下面介绍一个启发式的算法，其思想的模拟图遍历，并且给当前节点维护一个计数器 (counter), 计数器的数值是依赖于该节点并且尚未完成操作的的节点数量</p>
<img src="/optimizing-memory-consumption-in-deep-learning/alloc_step.png" class="" width="600">
<ul>
<li>在出度为 1 的节点上可以进行 In-Place 操作</li>
<li>在 counter&#x3D;&#x3D;0 之后可以把内存回收 (放到右上角的矩形框中)</li>
<li>需要新内存的时候，要么从右上角矩形框中取，要么申请新的内存。</li>
</ul>
<p>注意：在上面的模拟中，并没有真正分配内存，而是记录下来每个节点需要多少内存，然后，在模拟完成之后，一次性进行内存申请，申请内存大小是内需需求的上确界.(这是因为，每次申请内存的开销是一样的，与其每个节点都申请 (或释放) 内存，不如，最后一次性把所需要的内存全部申请到)</p>
<h1 id="Static-vs-Dynamic-Allocation"><a href="#Static-vs-Dynamic-Allocation" class="headerlink" title="Static vs. Dynamic Allocation"></a>Static vs. Dynamic Allocation</h1><p>上面的策略准确模拟了交互式语言如 Python 中的内存分配过程。其中 counter 是其引用计数，当引用计数变成 0 的时候进行内存回收。从这个意义上说，我们使用动态内存分配的方法创建了一个静态内存分配方案。可是，是否可以完全动态的分配和回收内存了？</p>
<p>其中主要的不同是，静态的内存分配只需要做一次，因此，复杂的内存分配算法是可以接受的。例如，可以搜索与需求的内存大小相似的内存块。而且，内存的分配对图来说是可感知的 (下一节讲到). 如果使用动态内存分配，那么，就需要非常快速的内存分配和回收策略。</p>
<p>对于计划使用动态内存分配方法的情况，有个技巧是：永远不要做没有必要的对象引用。例如，把所有的节点组织成一个链表形式并且存储在图对象中，结果是，所有的节点的引用计数都不会被释放，并且没有存储空间。</p>
<h1 id="Memory-Allocation-for-Parallel-Operations"><a href="#Memory-Allocation-for-Parallel-Operations" class="headerlink" title="Memory Allocation for Parallel Operations"></a>Memory Allocation for Parallel Operations</h1><p>前面讨论了如何对计算图进行静态的内存分配。但是，对于并行计算的内存优化会有新的问题，因为在并行计算中，资源的共享和并行计算是两个相互矛盾的状态。增加并行度意味着要减少资源共享，增加资源共享意味着要减少并行度。例如下面这个对于同一个图的两种不同的内存优化方案：</p>
<img src="/optimizing-memory-consumption-in-deep-learning/parallel_alloc.png" class="" width="600">
<p>两个分配方案，如果顺序地计算 A[1] 到 A[8] 都是可行的，但是，如果考虑到并行的状态，右边的方案显然更好，因为，在左边的方案中，A[2] 和 A[5] 是无法并行的。<br>对于并行计算，需要夹被小心。</p>
<h2 id="Be-Correct-and-Safe-First"><a href="#Be-Correct-and-Safe-First" class="headerlink" title="Be Correct and Safe First"></a>Be Correct and Safe First</h2><p>内存分配的正确性。要求要考虑到隐式的内存共享中的依赖。一种解决方法是，把隐式的依赖加入到图中 (通过边的连接), 另外一种更直接的方案是，如果执行引擎是修改可感知的，直接把操作放到队列中，写入到相同的 variable tag 中 (相同的 tag 表示相同的内存区域)<br>内存分配的安全性。要求不要在可并行的节点之间进行内存共享。虽然在内存优化要求高的情况下这不是理想的选择，而且，在同一个 GPU 上有多个计算流的时候，优化带来的收益并不是很多。但是，这不失为保证安全性的一种简单有效的方法。</p>
<h2 id="Try-to-Allow-More-Parallelization"><a href="#Try-to-Allow-More-Parallelization" class="headerlink" title="Try to Allow More Parallelization"></a>Try to Allow More Parallelization</h2><p>通过以上讨论，现在可以比较安全的进行优化了。其中心思想是在无法并行的节点之间进行内存共享。其中一种方式是构建一个祖先关系图，然后在分配内存的时候搜索该图，这种方案的时间复杂度是 \(O(n^2)\). 另外一种方案是启发式的，例如对图中的边进行染色。如下图所示，在图中找到最长的路径，然后把这个路径涂相同的颜色，然后继续。</p>
<img src="/optimizing-memory-consumption-in-deep-learning/graph_color.png" class="" width="600">
<p>完成上色之后，只允许 (或者说鼓励) 颜色相同的节点之间进行共享。这是祖先图的更强条件的特例，但是，其找到前 <code>k</code> 个路径的时间复杂度只有 \(O(n)\)</p>
<p>这应该不是唯一的方法，其它的方式肯定也是存在的。</p>
<h1 id="How-Much-Can-you-Save"><a href="#How-Much-Can-you-Save" class="headerlink" title="How Much Can you Save"></a>How Much Can you Save</h1><p>上面讨论了深度学习模型内存压缩的方法，但是，具体的效果怎么样呢？<br>在应经对 big operator 进行优化的粗粒度的图中，上述方法大约可以节省一般的内存。如果使用优化了的 symbolic 的库的话，效果会更好。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/MXNet/" rel="tag"># MXNet</a>
              <a href="/tags/HPC/" rel="tag"># HPC</a>
              <a href="/tags/Framework/" rel="tag"># Framework</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/resilio-sync/" rel="prev" title="使用 Resilio Sync 进行同步">
      <i class="fa fa-chevron-left"></i> 使用 Resilio Sync 进行同步
    </a></div>
      <div class="post-nav-item">
    <a href="/programming-models-for-deep-learning/" rel="next" title="深度学习中的编程模型">
      深度学习中的编程模型 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Computation-Graph"><span class="nav-number">1.</span> <span class="nav-text">Computation Graph</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-Can-Be-Optimized"><span class="nav-number">2.</span> <span class="nav-text">What Can Be Optimized</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#In-Place-Operations"><span class="nav-number">2.1.</span> <span class="nav-text">In-Place Operations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Standard-Memory-Sharing"><span class="nav-number">2.2.</span> <span class="nav-text">Standard Memory Sharing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Example-of-Real-Neural-Network-Allocation"><span class="nav-number">2.3.</span> <span class="nav-text">Example of Real Neural Network Allocation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Memory-Allocation-Algorithm"><span class="nav-number">3.</span> <span class="nav-text">Memory Allocation Algorithm</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Static-vs-Dynamic-Allocation"><span class="nav-number">4.</span> <span class="nav-text">Static vs. Dynamic Allocation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Memory-Allocation-for-Parallel-Operations"><span class="nav-number">5.</span> <span class="nav-text">Memory Allocation for Parallel Operations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Be-Correct-and-Safe-First"><span class="nav-number">5.1.</span> <span class="nav-text">Be Correct and Safe First</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Try-to-Allow-More-Parallelization"><span class="nav-number">5.2.</span> <span class="nav-text">Try to Allow More Parallelization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#How-Much-Can-you-Save"><span class="nav-number">6.</span> <span class="nav-text">How Much Can you Save</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">yushu.gao</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">92</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">32</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">94</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yushu.gao</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  


<script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script>
<style>
.geDiagramContainer { width: 100% !important; }
</style>
</body>

</html>
